<!DOCTYPE html>
<html lang='en'>

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>PyTorch入门 - LXA</title>

  
    <meta name="description" content="深度学习的入门部分">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch入门">
<meta property="og:url" content="http://lxafate.top/2023/01/20/PyTorch%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="LXA">
<meta property="og:description" content="深度学习的入门部分">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-01-20T07:26:25.943Z">
<meta property="article:modified_time" content="2023-01-27T08:48:25.741Z">
<meta property="article:author" content="LXA">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary_large_image">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  
    <link rel="shortcut icon" href="/assets/logo.png">
  

  

  


  
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css" />
    
  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="avatar" href="/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/assets/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">LXA</div><div class="sub normal cap">变得更好不只是为了输赢</div><div class="sub hover cap" style="opacity:0"> 而是让变得更好成为生命中的一部分</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">博客</a><a class="nav-item" href="/anime/">动画下载</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">No Results!</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">PyTorch入门</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%E9%83%A8%E5%88%86"><span class="toc-text">环境准备部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A8%E5%B0%BC%E7%8E%9B%E7%8B%97%E7%9A%84%E9%83%A8%E5%88%86%EF%BC%8CPytorch%E4%B8%8B%E8%BD%BD%E5%B7%A8%E6%85%A2"><span class="toc-text">巨尼玛狗的部分，Pytorch下载巨慢</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEPyCharm"><span class="toc-text">配置PyCharm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-text">随机梯度下降算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E7%9C%9F%E7%9A%84pytorch%E6%9D%A5%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%EF%BC%89"><span class="toc-text">线性回归（真的pytorch来解决问题）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#API%E4%BB%8B%E7%BB%8D"><span class="toc-text">API介绍</span></a></li></ol></li></ol></div></div></widget>




</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://space.bilibili.com/399644172" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/bilibili.png"/></a><a class="social" href="https://github.com/lxaDust" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/github.png"/></a></div></footer>

    </aside>
    <div class='l_main'>
      

      

    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });
      </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });
      </script>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>




<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">Home</a><span class="sep"></span><a class="cap breadcrumb" href="/">Blog</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E7%A8%8B%E5%BA%8F/">程序</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/%E7%A8%8B%E5%BA%8F/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div id="post-meta">Posted on&nbsp;<time datetime="2023-01-20T07:26:25.943Z">2023-01-20</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>PyTorch入门</span></h1>
<p>深度学习的入门部分</p>
<span id="more"></span>

<h2 id="环境准备部分"><a href="#环境准备部分" class="headerlink" title="环境准备部分"></a>环境准备部分</h2><div class="tag-plugin note" color="warning"><div class="body"><p>Python 3.9<br>Anaconda<br>Cuda 10.0<br>开发工具: PyCharm</p></div></div>

<h3 id="巨尼玛狗的部分，Pytorch下载巨慢"><a href="#巨尼玛狗的部分，Pytorch下载巨慢" class="headerlink" title="巨尼玛狗的部分，Pytorch下载巨慢"></a>巨尼玛狗的部分，Pytorch下载巨慢</h3><p>我TM真是服了国内的傻逼服务器和镜像，<del>你妈的</del>下东西是真tm慢啊</p>
<mark class="tag-plugin mark" color="dark">用管理员身份打开cmd</mark>添加配置:
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>

<p><del>同时添加几个第三方镜像配置</del><br>更新一手，没吊用了，挂个梯子硬下吧<br>查看已经安装的相关镜像源:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --show</span><br></pre></td></tr></table></figure>
<mark class="tag-plugin mark" color="dark">清除所有额外安装的镜像源</mark>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda config --remove-key channels</span><br></pre></td></tr></table></figure>

<mark class="tag-plugin mark" color="red">记得要挂美爹的节点</mark>


<h3 id="配置PyCharm"><a href="#配置PyCharm" class="headerlink" title="配置PyCharm"></a>配置PyCharm</h3><p>在pycharm中创建项目时，记得Python编辑器要选conda中的，不然找不到那个傻逼<mark class="tag-plugin mark" color="dark">torch</mark><br>验证pytorch是否安装好:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;gpu:&#x27;</span>,torch.cuda.is_available())</span><br></pre></td></tr></table></figure>


<h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><p>示例：</p>
<mark class="tag-plugin mark" color="dark">线性模型</mark>
<p>$ \widehat{y} &#x3D; x * \omega $<br>Training Loss(Error): $ loss &#x3D; (\widehat{y} - y)^2 &#x3D; (x * \omega - y)^2 $<br>–&gt;<br>Mean Square Error:$cost &#x3D; \frac{1}{N}\sum_{n&#x3D;1}^N (\widehat{y_{n}} - y_n)^2  $</p>
<details class="tag-plugin folding" ><summary><span>普通的计算方式</span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred =  forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y) <span class="comment">#(y_pred - y)^2</span></span><br><span class="line"></span><br><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,w)</span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val,y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        y_pred_val = forward(x_val) <span class="comment"># 预测的值（以当前循环中的w为准）</span></span><br><span class="line">        loss_val = loss(x_val,y_val) <span class="comment"># 当前w为准下，y的偏差的平方（这里面会再将上面一行函数调用一遍，参数也相同）</span></span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>,x_val,y_val,y_pred_val,loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>,l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(w_list,mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div></details>



<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><mark class="tag-plugin mark" color="dark">Gradient Descent</mark>
<p>核心在于<mark class="tag-plugin mark" color="dark">下降</mark>,在上面的题目中，绘制出的图像是一个y轴代表损失值、x轴代表w值的图像，则损失值越少，就代表当前的w越精确，所以要在函数图像上下降到y损失值最低的那个点。<br>这就是下降</p>
<mark class="tag-plugin mark" color="dark">梯度</mark>梯度部分，就是上面图像中每个点处代表的斜率$ \frac{\sigma cost}{\sigma \omega} $,斜率大于0时，w加上斜率损失值会增大，斜率小于0时也是，所以我们要用减去斜率的方式，也就是反向行走
<p>$$<br>w &#x3D; w -\alpha \frac{\sigma cost}{\sigma \omega}<br>$$<br>其中$ \alpha $代表学习率，相当于控制每一步要走多远<br>同样用于解上面问题的代码:</p>
<details class="tag-plugin folding" ><summary><span>梯度下降解法</span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">e_list = []</span><br><span class="line">w_list = []</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span> <span class="comment"># 初始值</span></span><br><span class="line"><span class="comment"># TODO 返回测试值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO 返回偏差值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs,ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        cost += (y - y_pred)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost/<span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO 返回梯度值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs,ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        y_pred = forward(x)</span><br><span class="line">        grad += <span class="number">2</span> * x * (y_pred - y)</span><br><span class="line">    <span class="keyword">return</span> grad/<span class="built_in">len</span>(xs)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict(before training)&#x27;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data,y_data)</span><br><span class="line">    grad_val = gradient(x_data,y_data)</span><br><span class="line">    w -= <span class="number">0.01</span> * grad_val</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    e_list.append(epoch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>,epoch,<span class="string">&#x27;w=&#x27;</span>,w,<span class="string">&#x27;loss=&#x27;</span>,cost_val)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict (after training)&#x27;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line">plt.plot(e_list,w_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div></details>

<h3 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h3><mark class="tag-plugin mark" color="dark">Stochastic Gradient Descent(SGD)</mark>
<p>最基本的随机梯度下降，每次更新参数只使用1个样本<br>本题中就是在每一轮中，对每一个样本都进行参数的更新<br>此算法不利于并行计算，因为每次进行参数的更新之后，下一个样本就要用到这个更新后的参数，而不会等到下一轮。</p>
<details class="tag-plugin folding" ><summary><span>SGD代码</span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TODO 随机梯度下降算法</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = <span class="number">1.0</span></span><br><span class="line">e_list = []</span><br><span class="line">w_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y - y_pred)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (x * w - y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predict(before training)&#x27;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        grad_val = gradient(x,y)</span><br><span class="line">        w -= <span class="number">0.01</span> * grad_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad:&quot;</span>,x,y,grad_val)</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    e_list.append(epoch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress:&quot;</span>,epoch,<span class="string">&quot;w=&quot;</span>,w,<span class="string">&quot;loss=&quot;</span>,l)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predict (after training)&quot;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br><span class="line">plt.plot(e_list,w_list)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div></details>


<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><mark class="tag-plugin mark" color="dark">Back Propagation(反向传播)</mark>
<div class="tag-plugin note" color="warning"><div class="body"><p>前向传播通过训练数据和权重参数计算输出结果；<br>反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新</p></div></div>


<p>函数<br>$$<br>y &#x3D; wx<br>$$<br>的反向传播代码:</p>
<details class="tag-plugin folding" ><summary><span></span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#TODO BackPropagation 反向传播</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line">w = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">w.requires_grad = <span class="literal">True</span> <span class="comment"># 一定要为True，不然不会计算梯度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span>  <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>,<span class="number">4</span>,forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad:&quot;</span>,x,y,w.grad.item())</span><br><span class="line">        w.data -= <span class="number">0.01</span> * w.grad.data</span><br><span class="line"></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress:&quot;</span>,epoch,l.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>,<span class="number">4</span>,forward(<span class="number">4</span>).item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></details>

<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>$$<br>y &#x3D; \omega_1 x^2 + \omega_2 x + b<br>$$</p>
<p>代码:</p>
<details class="tag-plugin folding" ><summary><span></span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_data = [<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>]</span><br><span class="line">y_data = [<span class="number">1.0</span>,<span class="number">6.0</span>,<span class="number">15.0</span>]</span><br><span class="line"></span><br><span class="line">w1 = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">w2 = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">b = torch.tensor([<span class="number">2.0</span>])</span><br><span class="line">w1.requires_grad = <span class="literal">True</span></span><br><span class="line">w2.requires_grad = <span class="literal">True</span></span><br><span class="line">b.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forR1</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w1 * x * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forR2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w2 * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> forR1(x) + forR2(x) + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training):&quot;</span>,<span class="number">3</span>,forward(<span class="number">3</span>).item())</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        l.backward()</span><br><span class="line">        w1.data -= <span class="number">0.01</span> * w1.grad.data</span><br><span class="line">        w2.data -= <span class="number">0.01</span> * w2.grad.data</span><br><span class="line">        b.data -= <span class="number">0.01</span> * b.grad.data</span><br><span class="line"></span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress:&quot;</span>,epoch,l.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>,<span class="number">3</span>,forward(<span class="number">3</span>).item())</span><br><span class="line"></span><br></pre></td></tr></table></figure></div></details>
<p>这个代码还是有缺陷的，最后的预测结果会有偏差。</p>
<h2 id="线性回归（真的pytorch来解决问题）"><a href="#线性回归（真的pytorch来解决问题）" class="headerlink" title="线性回归（真的pytorch来解决问题）"></a>线性回归（真的pytorch来解决问题）</h2><details class="tag-plugin folding" ><summary><span>代码</span></summary><div class="body"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">x_data = torch.tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line">model = LinearModel()</span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch,loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b=&#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred=&#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure></div></details>

<h3 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h3><div class="tag-plugin note" color="red"><div class="body"><p>torch.nn.Linear(1,1)</p></div></div>
<p>官方文档:</p>
<div class="tag-plugin link dis-select"><a class="link-card rich" title="" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear" target="_blank" rel="external nofollow noopener noreferrer" cardlink autofill="title,icon,desc"><div class="top"><div class="lazy img" data-bg="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.4/link/8f277b4ee0ecd.svg"></div><span class="cap link fs12">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span></div><div class="bottom"><span class="title">https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear</span><span class="cap desc fs12"></span></div></a></div>
<mark class="tag-plugin mark" color="dark">Class:</mark>  torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)
<mark class="tag-plugin mark" color="dark">parameters:</mark>
<ul>
<li><p>in_features (int) – size of each input sample</p>
</li>
<li><p>out_features (int) – size of each output sample</p>
</li>
<li><p>bias (bool) – If set to False, the layer will not learn an additive bias. Default: True</p>
</li>
</ul>


<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>License</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"></div><div class="item" id="next"><div class="note">Older</div><a href="/2023/01/17/Git/">git与idea的学习</a></div></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a href="/">@anonymity</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.18.5';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5';
  stellar.config = {
    date_suffix: {
      just: 'Just',
      min: 'minutes ago',
      hour: 'hours ago',
      day: 'days ago',
      month: 'months ago',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
</body>
</html>
